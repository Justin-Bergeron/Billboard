{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This scrapes all the years from the Billboard site\n",
    "import time\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from contextlib import suppress\n",
    "from selenium import webdriver\n",
    "import json\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options     #These lines are to add uBlock origin to the headless browser\n",
    "path_to_extension = '1.16.14_0'\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('load-extension=' + path_to_extension) \n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "driver.create_options()\n",
    "#driver.get(\"http://www.google.com\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize browser\n",
    "def init_browser():\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, options=chrome_options, headless=False)\n",
    "browser = init_browser()\n",
    "\n",
    "urlList = []\n",
    "urlListClean = []\n",
    "year = ['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', \n",
    "        '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', \n",
    "        '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', \n",
    "        '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', \n",
    "        '2016', '2017', '2018'] \n",
    "\n",
    "\n",
    "\n",
    "for y in year:\n",
    "    # Visit billboard site\n",
    "    url = \"https://www.billboard.com/archive/charts/%s/hot-100\" % y\n",
    "    browser.visit(url)\n",
    "    print(url)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Scrape page into soup\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table', class_ = 'archive-table')\n",
    "    url = table.find_all('a', href = True)         #, class_ = 'sl-spot-list__ref'\n",
    "    urlList.extend(url)\n",
    "    for a in url:\n",
    "        urlListClean.append(a['href'])\n",
    "    \n",
    "print(urlListClean)    \n",
    "with open('URLs.json', 'w') as outfile:\n",
    "      json.dump(urlListClean, outfile)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'urlListClean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-dfab813f7681>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murlListClean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatetime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1960-01-04\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'urlListClean' is not defined"
     ]
    }
   ],
   "source": [
    "#format the datetime from scrape\n",
    "print(urlListClean[0])\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "d = \"1960-01-04\"\n",
    "date_obj = datetime.strptime(d, \"%Y-%m-%d\")\n",
    "formatted_date = datetime.strftime(date_obj, \"%B %d, %Y\")\n",
    "print(formatted_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second scrape to gather song data from the top ten artists for each week on the billboard site. \n",
    "\n",
    "# Visit billboard site\n",
    "\n",
    "# with open(\"URLs.json\",\"r\") as f:\n",
    "#     urlListClean = f.read()\n",
    "#     print(urlListClean)[0]\n",
    "    \n",
    "# data = json.load('URLs.json')    \n",
    "\n",
    "forJson = []\n",
    "\n",
    "for u in urlListClean[2000:3040]:\n",
    "    url = \"https://www.billboard.com%s\" %u\n",
    "    browser.visit(url)\n",
    "    print(url)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Scrape page into soup\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    #Find the top ten songs\n",
    "    songs = []\n",
    "    song_search = soup.find_all('span', class_=\"chart-list-item__title-text\")\n",
    "    for a in song_search[:9]:\n",
    "        songs.append(a.text.strip())\n",
    "        \n",
    "\n",
    "    #Find the top ten artists\n",
    "    artists = []\n",
    "    artist_search = soup.find_all('div', class_=\"chart-list-item__artist\")\n",
    "    for a in artist_search[:9]:\n",
    "        artists.append(a.text.strip())    \n",
    "    \n",
    "    #Weeks on chart\n",
    "    weeks_on_chart = []\n",
    "    chart_search = soup.find_all('div', class_=\"chart-list-item__weeks-on-chart\")\n",
    "    for a in chart_search[:9]:\n",
    "        weeks_on_chart.append(a.text.strip())      \n",
    "    \n",
    "    weekOf = 0\n",
    "    #print(weekOf)\n",
    "    \n",
    "    chart =[\n",
    "                {'Week of': soup.find(class_ =\"chart-detail-header__date-selector-button\").text.strip() },\n",
    "                {'Position': 1, 'Song':soup.find(class_ =\"chart-number-one__title\").text, 'Artist':(soup.find(\"div\", class_=\"chart-number-one__artist\").text.strip()), \"Weeks on Chart\":soup.find(\"div\", class_=\"chart-number-one__weeks-on-chart\").text},\n",
    "                {'Position': 2, 'Song':songs[0], 'Artist':artists[0], 'Weeks on Chart': weeks_on_chart[0]},\n",
    "                {'Position': 3, 'Song':songs[1], 'Artist':artists[1], 'Weeks on Chart': weeks_on_chart[1]},\n",
    "                {'Position': 4, 'Song':songs[2], 'Artist':artists[2], 'Weeks on Chart': weeks_on_chart[2]},\n",
    "                {'Position': 5, 'Song':songs[3], 'Artist':artists[3], 'Weeks on Chart': weeks_on_chart[3]},\n",
    "                {'Position': 6, 'Song':songs[4], 'Artist':artists[4], 'Weeks on Chart': weeks_on_chart[4]},\n",
    "                {'Position': 7, 'Song':songs[5], 'Artist':artists[5], 'Weeks on Chart': weeks_on_chart[5]},\n",
    "                {'Position': 8, 'Song':songs[6], 'Artist':artists[6], 'Weeks on Chart': weeks_on_chart[6]},\n",
    "                {'Position': 9, 'Song':songs[7], 'Artist':artists[7], 'Weeks on Chart': weeks_on_chart[7]},\n",
    "                {'Position': 10, 'Song':songs[8], 'Artist':artists[8], 'Weeks on Chart': weeks_on_chart[8]}]\n",
    "\n",
    "    week_total = {weekOf : chart}\n",
    "    #print(week_total)\n",
    "    forJson.append(week_total)\n",
    "    \n",
    "#Just put it to a json when it's done\n",
    "with open('billboard[2000:3040].json', 'a') as outfile:      #'a' so it appends the file\n",
    "   json.dump(forJson, outfile)\n",
    "    \n",
    "print('2000:3040')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just put it to a json when it's done\n",
    "with open('billboard.json', 'a') as outfile:      #'a' so it appends the file\n",
    "   json.dump(forJson, outfile)\n",
    "    \n",
    "print('k')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
