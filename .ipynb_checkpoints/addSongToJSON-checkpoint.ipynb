{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This scrapes all the years from the Billboard site\n",
    "\n",
    "import time\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from contextlib import suppress\n",
    "from selenium import webdriver\n",
    "import json\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options     #These lines are to add uBlock origin to the headless browser\n",
    "path_to_extension = '1.16.14_0'\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('load-extension=' + path_to_extension) \n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "driver.create_options()\n",
    "#driver.get(\"http://www.google.com\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize browser    create a list of all billboard years\n",
    "def init_browser():\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, options=chrome_options, headless=False)\n",
    "browser = init_browser()\n",
    "\n",
    "urlList = []\n",
    "urlListClean = []\n",
    "year = ['1960', '1961', '1962', '1963', '1964', '1965', '1966', '1967', '1968', '1969', '1970', '1971', '1972', '1973', \n",
    "        '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981', '1982', '1983', '1984', '1985', '1986', '1987', \n",
    "        '1988', '1989', '1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', \n",
    "        '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', \n",
    "        '2016', '2017', '2018'] \n",
    "\n",
    "\n",
    "\n",
    "for y in year:\n",
    "    # Visit billboard site\n",
    "    url = \"https://www.billboard.com/archive/charts/%s/hot-100\" % y\n",
    "    browser.visit(url)\n",
    "    print(url)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Scrape page into soup\n",
    "    html = browser.html\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "    table = soup.find('table', class_ = 'archive-table')\n",
    "    url = table.find_all('a', href = True)         #, class_ = 'sl-spot-list__ref'\n",
    "    urlList.extend(url)\n",
    "    for a in url:\n",
    "        urlListClean.append((a['href'])[-10:])\n",
    "    \n",
    "print(urlListClean)    \n",
    "with open('URLdates.json', 'w') as outfile:\n",
    "      json.dump(urlListClean, outfile)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
