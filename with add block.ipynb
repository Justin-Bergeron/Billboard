{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "from contextlib import suppress\n",
    "from selenium import webdriver\n",
    "import json\n",
    "\n",
    "from selenium.webdriver.chrome.options import Options     #These lines are to add uBlock origin to the headless browser\n",
    "path_to_extension = '1.16.14_0'\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('load-extension=' + path_to_extension) \n",
    "driver = webdriver.Chrome(chrome_options=chrome_options)\n",
    "driver.create_options()\n",
    "#driver.get(\"http://www.google.com\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize browser\n",
    "def init_browser():\n",
    "    executable_path = {\"executable_path\": \"/usr/local/bin/chromedriver\"}\n",
    "    return Browser(\"chrome\", **executable_path, options=chrome_options, headless=False)\n",
    "\n",
    "# urlList = []\n",
    "# year = ['1960', '1961', '1963']\n",
    "browser = init_browser()\n",
    "\n",
    "\n",
    "# for y in year:\n",
    "#     # Visit billboard site\n",
    "#     url = \"https://www.billboard.com/archive/charts/%s/hot-100\" % y\n",
    "#     browser.visit(url)\n",
    "#     print(url)\n",
    "\n",
    "    \n",
    "    \n",
    "#     # Scrape page into soup\n",
    "#     html = browser.html\n",
    "#     soup = BeautifulSoup(html, 'html.parser')\n",
    "    \n",
    "#     table = soup.find('table', class_ = 'archive-table')\n",
    "#     url = table.find_all('a', href = True)         #, class_ = 'sl-spot-list__ref'\n",
    "#     urlList.append(url)\n",
    "#     urlList.append(\"****************************88888**\")\n",
    "\n",
    "# print(urlList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.billboard.com/charts/hot-100/1968-01-06\n",
      "January 6, 1968\n",
      "{'January 6, 1968': [{'Week of': 'January 6, 1968'}, {'Position': 1, 'Song': 'Hello Goodbye', 'Artist': 'The Beatles', 'Weeks at 1': '2', 'Weeks on Chart': '6'}, {'Position': 2, 'Song': 'Daydream Believer', 'Artist': 'The Monkees', 'Weeks on Chart': '8'}, {'Position': 3, 'Song': 'Judy In Disguise (With Glasses)', 'Artist': 'John Fred And The Playboys', 'Weeks on Chart': '7'}, {'Position': 4, 'Song': 'I Heard It Through The Grapevine', 'Artist': 'Gladys Knight And The Pips', 'Weeks on Chart': '12'}, {'Position': 5, 'Song': 'Woman, Woman', 'Artist': 'The Union Gap Featuring Gary Puckett', 'Weeks on Chart': '8'}, {'Position': 6, 'Song': 'I Second That Emotion', 'Artist': 'Smokey Robinson & The Miracles', 'Weeks on Chart': '10'}, {'Position': 7, 'Song': 'Chain Of Fools', 'Artist': 'Aretha Franklin', 'Weeks on Chart': '5'}, {'Position': 8, 'Song': 'Bend Me, Shape Me', 'Artist': 'The American Breed', 'Weeks on Chart': '6'}, {'Position': 9, 'Song': 'Boogaloo Down Broadway', 'Artist': 'The Fantastic Johnny C', 'Weeks on Chart': '14'}, {'Position': 0, 'Song': 'Skinny Legs And All', 'Artist': 'Joe Tex', 'Weeks on Chart': '11'}]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Visit billboard site\n",
    "url = \"https://www.billboard.com/charts/hot-100/1968-01-06\"\n",
    "browser.visit(url)\n",
    "print(url)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Scrape page into soup\n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#Find the top ten songs\n",
    "songs = []\n",
    "song_search = soup.find_all('span', class_=\"chart-list-item__title-text\")\n",
    "for a in song_search[:9]:\n",
    "    songs.append(a.text.strip())\n",
    "\n",
    "    #Find the top ten artists\n",
    "artists = []\n",
    "artist_search = soup.find_all('div', class_=\"chart-list-item__artist\")\n",
    "for a in artist_search[:9]:\n",
    "    artists.append(a.text.strip())    \n",
    "    \n",
    "    #Weeks on chart\n",
    "weeks_on_chart = []\n",
    "chart_search = soup.find_all('div', class_=\"chart-list-item__weeks-on-chart\")\n",
    "for a in chart_search[:9]:\n",
    "    weeks_on_chart.append(a.text.strip())      \n",
    "    \n",
    "fuckthis = soup.find(class_ =\"chart-detail-header__date-selector-button\").text.strip()\n",
    "print(fuckthis)\n",
    "    \n",
    "Week_of_date =[\n",
    "            {'Week of': soup.find(class_ =\"chart-detail-header__date-selector-button\").text.strip() },\n",
    "            {'Position': 1, 'Song':soup.find(class_ =\"chart-number-one__title\").text, 'Artist':(soup.find(\"div\", class_=\"chart-number-one__artist\").text.strip()), \"Weeks at 1\":soup.find(\"div\", class_=\"chart-number-one__weeks-at-one\").text, \"Weeks on Chart\":soup.find(\"div\", class_=\"chart-number-one__weeks-on-chart\").text},\n",
    "            {'Position': 2, 'Song':songs[0], 'Artist':artists[0], 'Weeks on Chart': weeks_on_chart[0]},\n",
    "            {'Position': 3, 'Song':songs[1], 'Artist':artists[1], 'Weeks on Chart': weeks_on_chart[1]},\n",
    "            {'Position': 4, 'Song':songs[2], 'Artist':artists[2], 'Weeks on Chart': weeks_on_chart[2]},\n",
    "            {'Position': 5, 'Song':songs[3], 'Artist':artists[3], 'Weeks on Chart': weeks_on_chart[3]},\n",
    "            {'Position': 6, 'Song':songs[4], 'Artist':artists[4], 'Weeks on Chart': weeks_on_chart[4]},\n",
    "            {'Position': 7, 'Song':songs[5], 'Artist':artists[5], 'Weeks on Chart': weeks_on_chart[5]},\n",
    "            {'Position': 8, 'Song':songs[6], 'Artist':artists[6], 'Weeks on Chart': weeks_on_chart[6]},\n",
    "            {'Position': 9, 'Song':songs[7], 'Artist':artists[7], 'Weeks on Chart': weeks_on_chart[7]},\n",
    "            {'Position': 0, 'Song':songs[8], 'Artist':artists[8], 'Weeks on Chart': weeks_on_chart[8]}]\n",
    "\n",
    "hell = {fuckthis : Week_of_date}\n",
    "print(hell)\n",
    "\n",
    "\n",
    "#print(Week_of_date)\n",
    "#print(artists)\n",
    "#print(weeks_on_chart)\n",
    "    \n",
    "#jsonData = json.dump(hell, data_file)\n",
    "#print(jsonData)\n",
    "with open('billboard.json', 'w') as outfile:\n",
    "      json.dump(hell, outfile)\n",
    "    \n",
    "# parsed = json.loads('billboard.json')\n",
    "# print(json.dump(parsed, indent=4, sort_keys=True))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
